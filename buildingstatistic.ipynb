{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "path_data = '../StatisticLearning/data/'\n",
    "path_train = path_data + \"train.csv\"\n",
    "path_test = path_data + \"test.csv\"\n",
    "path_building = path_data + \"building_metadata.csv\"\n",
    "path_weather_train = path_data + \"weather_train.csv\"\n",
    "path_weather_test = path_data + \"weather_test.csv\"\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "myfavouritenumber = 0\n",
    "seed = myfavouritenumber\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 289.19 Mb (53.1% reduction)\n",
      "Mem. usage decreased to  0.02 Mb (72.6% reduction)\n",
      "Mem. usage decreased to  3.07 Mb (68.1% reduction)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_train = pd.read_csv(path_train)\n",
    "\n",
    "building = pd.read_csv(path_building)\n",
    "le = LabelEncoder()\n",
    "building.primary_use = le.fit_transform(building.primary_use)\n",
    "\n",
    "weather_train = pd.read_csv(path_weather_train)\n",
    "df_train = reduce_mem_usage(df_train, True)\n",
    "building = reduce_mem_usage(building, True)\n",
    "weather_train = reduce_mem_usage(weather_train, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_data(X, building_data, weather_data, test=False):\n",
    "    \"\"\"\n",
    "    Preparing final dataset with all features.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = X.merge(building_data, on=\"building_id\", how=\"left\")\n",
    "    X = X.merge(weather_data, on=[\"site_id\", \"timestamp\"], how=\"left\")\n",
    "    \n",
    "    X.timestamp = pd.to_datetime(X.timestamp, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    X.square_feet = np.log1p(X.square_feet)\n",
    "    \n",
    "    if not test:\n",
    "        X.sort_values(\"timestamp\", inplace=True)\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    holidays = [\"2016-01-01\", \"2016-01-18\", \"2016-02-15\", \"2016-05-30\", \"2016-07-04\",\n",
    "                \"2016-09-05\", \"2016-10-10\", \"2016-11-11\", \"2016-11-24\", \"2016-12-26\",\n",
    "                \"2017-01-01\", \"2017-01-16\", \"2017-02-20\", \"2017-05-29\", \"2017-07-04\",\n",
    "                \"2017-09-04\", \"2017-10-09\", \"2017-11-10\", \"2017-11-23\", \"2017-12-25\",\n",
    "                \"2018-01-01\", \"2018-01-15\", \"2018-02-19\", \"2018-05-28\", \"2018-07-04\",\n",
    "                \"2018-09-03\", \"2018-10-08\", \"2018-11-12\", \"2018-11-22\", \"2018-12-25\",\n",
    "                \"2019-01-01\"]\n",
    "    \n",
    "    X[\"hour\"] = X.timestamp.dt.hour\n",
    "    X[\"weekday\"] = X.timestamp.dt.weekday\n",
    "    X[\"is_holiday\"] = (X.timestamp.dt.date.astype(\"str\").isin(holidays)).astype(int)\n",
    "    \n",
    "    drop_features = [\"timestamp\", \"sea_level_pressure\", \"wind_direction\", \"wind_speed\"]\n",
    "\n",
    "    X.drop(drop_features, axis=1, inplace=True)\n",
    "\n",
    "    if test:\n",
    "        row_ids = X.row_id\n",
    "        X.drop(\"row_id\", axis=1, inplace=True)\n",
    "        return X, row_ids\n",
    "    else:\n",
    "        y = np.log1p(X.meter_reading)\n",
    "        X.drop(\"meter_reading\", axis=1, inplace=True)\n",
    "        return X, y\n",
    "\n",
    "X_train, y_train = prepare_data(df_train, building, weather_train)\n",
    "\n",
    "del df_train, weather_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with first half and validating on second half:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\TensorFlow\\lib\\site-packages\\lightgbm\\basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's rmse: 0.921114\tvalid_1's rmse: 1.3398\n",
      "[400]\ttraining's rmse: 0.867096\tvalid_1's rmse: 1.33407\n",
      "[600]\ttraining's rmse: 0.842078\tvalid_1's rmse: 1.33537\n",
      "Early stopping, best iteration is:\n",
      "[496]\ttraining's rmse: 0.855098\tvalid_1's rmse: 1.33353\n",
      "Building model with second half and validating on first half:\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's rmse: 0.90602\tvalid_1's rmse: 1.50684\n",
      "[400]\ttraining's rmse: 0.860636\tvalid_1's rmse: 1.50277\n",
      "[600]\ttraining's rmse: 0.835886\tvalid_1's rmse: 1.50042\n",
      "[800]\ttraining's rmse: 0.819647\tvalid_1's rmse: 1.50033\n",
      "Early stopping, best iteration is:\n",
      "[664]\ttraining's rmse: 0.830981\tvalid_1's rmse: 1.49966\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_half_1 = X_train[:int(X_train.shape[0] / 2)]\n",
    "X_half_2 = X_train[int(X_train.shape[0] / 2):]\n",
    "\n",
    "y_half_1 = y_train[:int(X_train.shape[0] / 2)]\n",
    "y_half_2 = y_train[int(X_train.shape[0] / 2):]\n",
    "\n",
    "categorical_features = [\"building_id\", \"site_id\", \"meter\", \"primary_use\", \"hour\", \"weekday\"]\n",
    "\n",
    "d_half_1 = lgb.Dataset(X_half_1, label=y_half_1, categorical_feature=categorical_features, free_raw_data=False)\n",
    "d_half_2 = lgb.Dataset(X_half_2, label=y_half_2, categorical_feature=categorical_features, free_raw_data=False)\n",
    "\n",
    "watchlist_1 = [d_half_1, d_half_2]\n",
    "watchlist_2 = [d_half_2, d_half_1]\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"num_leaves\": 40,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.85,\n",
    "    \"reg_lambda\": 2,\n",
    "    \"metric\": \"rmse\"\n",
    "}\n",
    "\n",
    "print(\"Building model with first half and validating on second half:\")\n",
    "model_half_1 = lgb.train(params, train_set=d_half_1, num_boost_round=1000, valid_sets=watchlist_1, verbose_eval=200, early_stopping_rounds=200)\n",
    "\n",
    "print(\"Building model with second half and validating on first half:\")\n",
    "model_half_2 = lgb.train(params, train_set=d_half_2, num_boost_round=1000, valid_sets=watchlist_2, verbose_eval=200, early_stopping_rounds=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 596.49 Mb (53.1% reduction)\n",
      "Mem. usage decreased to  6.08 Mb (68.1% reduction)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_test = pd.read_csv(path_test)\n",
    "weather_test = pd.read_csv(path_weather_test)\n",
    "\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "weather_test = reduce_mem_usage(weather_test)\n",
    "\n",
    "X_test, row_ids = prepare_data(df_test, building, weather_test, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.expm1(model_half_1.predict(X_test, num_iteration=model_half_1.best_iteration)) / 2\n",
    "\n",
    "del model_half_1\n",
    "gc.collect()\n",
    "\n",
    "pred += np.expm1(model_half_2.predict(X_test, num_iteration=model_half_2.best_iteration)) / 2\n",
    "    \n",
    "del model_half_2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " submission = pd.DataFrame({\"row_id\": row_ids, \"meter_reading\": np.clip(pred, 0, a_max=None)})\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
